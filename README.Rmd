---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# focus: Feature selectiOn and Clustering Using Stability

<!-- badges: start -->
<!-- badges: end -->

Implementation of stability-enhanced models for variable selection in multivariate regression, graphical and clustering models. 
These methods rely on resampling approaches to estimate selection (or co-membership) probability. 
Calibration of the models is done via maximisation of a stability score measuring the likelihood of informative (non-uniform) selection procedure.

## Installation

<!-- You can install the released version of focus from [CRAN](https://CRAN.R-project.org) with: -->

<!-- ``` r -->
<!-- install.packages("focus") # devtools::install_github("barbarabodinier/focus") -->
<!-- ``` -->

The development version can be downloaded from [GitHub](https://github.com/) and installed using the following command from a working directory containing the folder:

``` r
devtools::install("focus", upgrade="always")
```

## Variable selection

### Data simulation

A dataset with n=100 observations for p=50 potential predictors and a continuous outcome is simulated:

```{r}
library(focus)

# Data simulation
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50)

# Potential predictors
X <- simul$xdata
print(dim(X))

# Continuous outcome
Y <- simul$ydata
print(dim(Y))
```


### Stability selection

Stability selection in a regression framework is implemented in the function `VariableSelection()`. The predictor and outcome datasets are provided as input:

```{r results="hide"}
stab <- VariableSelection(xdata = X, ydata = Y)
print(stab)
```

Stability selection models are run for multiple pairs of parameters λ (controlling the sparsity of the underlying algorithm) and π (threshold in selection proportions). By default, stability selection is run in applied to LASSO regression, as implemented in `glmnet`. The grids of parameter values used in the run can be extracted using: 

```{r}
# First few penalty parameters
print(head(stab$Lambda))

# Grid of thresholds in selection proportion
print(stab$params$pi_list)

# Number of model pairs (i.e. number of visited stability selection models)
print(nrow(stab$Lambda) * length(stab$params$pi_list))
```


### Calibration 

The two parameters are jointly calibrated by maximising the stability score, measuring how unlikely it is that features are uniformly selected:

```{r fig.height=7, fig.width=9, out.width="80%", fig.align="center"}
par(mar = c(7, 5, 7, 6))
CalibrationPlot(stab)
summary(stab)
```

Visited penalty parameters λ are represented on the x-axis. The corresponding average number of selected features by the underlying algorithm (here, LASSO models) are reported on the z-axis and denoted by q. The different thresholds in selection proportions π are represented on the y-axis. The stability score obtained for different pairs of parameters (λ, π) are colour-coded and ranging from 0 to 1,200 on this example. 


### Outputs

Stably selected variables in the calibrated model are denoted by a "1" in:

```{r}
SelectedVariables(stab)
```

Selection proportions of the calibrated model can be extracted using:

```{r}
SelectionProportions(stab)
```

Selection proportions can be used to rank the variables by relevance in association with the outcome:

```{r fig.height=7, fig.width=12, out.width="100%", fig.align="center"}
plot(stab)
```


## Graphical modelling

### Data simulation

A dataset with n=100 observations of p=10 nodes with an underlying graph structure is simulated:

```{r}
# Data simulation
set.seed(1)
simul <- SimulateGraphical(n = 100, pk = 10, topology = "scale-free")

# Variables are nodes
X <- simul$data
print(dim(X))
```

### Stability selection

Stability selection for graphical modelling is implemented in `GraphicalModel()`. For sparser estimates, the model can be estimated under the constraint that the upper-bound of the expected number of falsely selected edges (PFER) is below a user-defined threshold:

```{r results="hide"}
stab <- GraphicalModel(xdata = X, PFER_thr = 10)
print(stab)
```


### Calibration 

As for variable selection, the stability selection graphical model is controlled by two parameters controlling the sparsity of the underlying algorithm and threshold in selection proportion. These parameters are jointly calibrated by maximising the stability score:

```{r fig.height=7, fig.width=9, out.width="80%", fig.align="center"}
par(mar = c(7, 5, 7, 6))
CalibrationPlot(stab)
summary(stab)
```

The effect of the constraint on the PFER can be visualised in this calibration plot. The white area corresponds to "forbidden" models, where the upper-bound of the expected number of parameters would exceed the value specified in PFER\_thr.


### Outputs

The calibrated graph can be visualised using:

```{r fig.height=9, fig.width=9, out.width="100%", fig.align="center"}
set.seed(1)
plot(stab)
```

The adjacency matrix of the calibrated stability selection graphical model can also be extracted using:

```{r}
Adjacency(stab)
```

And converted to an igraph object using:

```{r fig.height=9, fig.width=9, out.width="100%", fig.align="center"}
mygraph=Graph(Adjacency(stab))
mygraph
set.seed(1)
plot(mygraph)
```

## Clustering

### Data simulation

A dataset with k=3 clusters of participants sharing similar profiles along 5 out of 15 variables is simulated:

```{r fig.height=7, fig.width=7, out.width="60%", fig.align="center"}
# Data simulation
set.seed(1)
simul <- SimulateClustering(
  n = c(10, 10, 10), pk = 15,
  theta_xc = c(rep(1, 5), rep(0, 10)),
  ev_xc = c(rep(0.99, 5), rep(0, 10)),
)
X <- simul$data

# Visualisation of distances between participants
par(mar = c(5, 5, 5, 5))
Heatmap(
  mat = as.matrix(dist(simul$data)),
  colours = c("navy", "white", "red")
)

# Visualisation along two variables
plot(simul$data[,1:2], col=simul$theta)
```


### Consensus clustering

Consensus clustering is implemented in `Clustering()`. By default, sparse hierarchical clustering is used and applied on datasets with subsamples of the participants. The function takes the data as input:

```{r results="hide"}
stab <- Clustering(
  xdata = simul$data,
  Lambda = cbind(seq(1.1, 2, by = 0.1))
)
```


<!-- ### Calibration  -->

<!-- The calibration of sparse consensus clustering is conducted in two steps: (i) calibration of parameters controlling the variable selection, and (ii) calibration of parameters controlling the clusters, conditionally on the choice of hyper-parameter for the variable selection. -->

<!-- #### Variable selection -->

<!-- In sparse hierarchical clustering, the number of variables used for the clustering is controlled by a penalty parameter. The penalty and threshold in selection proportions for the variables are jointly calibrated by maximising the stability score: -->

<!-- ```{r fig.height=7, fig.width=9, out.width="80%", fig.align="center"} -->
<!-- par(mar = c(7, 5, 7, 6)) -->
<!-- CalibrationPlot(stab) -->
<!-- ``` -->

<!-- #### Identification of stable clusters -->

<!-- In addition, the consensus clustering model is defined by the number of clusters and a threshold in co-membership proportion. These two parameters are calibrated by maximising the stability score across models with the chosen penalty parameter: -->

<!-- ```{r fig.height=7, fig.width=9, out.width="80%", fig.align="center"} -->
<!-- par(mar = c(7, 5, 7, 6)) -->
<!-- CalibrationPlot(stab, clustering=TRUE) -->
<!-- ``` -->

### Outputs

#### Variable selection

The variables contributing to the clustering structure can be identified using:

```{r}
# Stable selection status
SelectedVariables(stab)

# Selection proportions
SelectionProportions(stab)
```

#### Identification of stable clusters

In this example, observations that are grouped in the same cluster in more than 90% (calibrated threshold) of the subsamples are considered as stable co-members. The stable co-membership status can be obtained from:

```{r}
myadjacency <- Adjacency(stab)
```

It can be interpreted like an adjacency matrix and visualised as a graph:

```{r fig.height=9, fig.width=9, out.width="100%", fig.align="center"}
mygraph <- Graph(myadjacency, node_colour = simul$theta)
set.seed(1)
plot(mygraph)
```

The stable clusters are defined as the connected components of this graph. Stable cluster membership can be obtained using:

```{r}
membership <- Clusters(stab)
print(membership)
```


## Dimensionality reduction

### Data simulation

A dataset with n=100 observations and p=10 variables that could be reconstructed from a sparse linear combination of orthogonal variables (Principal Components) is simulated:

```{r fig.height=7, fig.width=7, out.width="60%", fig.align="center"}
# Data simulation
set.seed(1)
simul <- SimulateComponents(n = 100, pk = c(3, 5, 4), v_within = c(0.8, 1), v_sign = -1)
X <- simul$data

# Visualisation of correlations between participants
par(mar = c(5, 5, 5, 5))
Heatmap(
  mat = cor(X),
  colours = c("navy", "white", "red"),
  legend_range = c(-1, 1)
)
```


### Stability selection

Stability selection for dimensionality reduction is implemented in `BiSelection()`. It can be used in combination with sparse Principal Component Analysis (sPCA) to recover the sparse set of variables contributing to the Principal Components (PCs). Stability selection can be applied on the first three PCs using: 

```{r results="hide"}
stab <- BiSelection(xdata = X, implementation = SparsePCA, ncomp = 3)
```


### Calibration 

As for models presented in previous sections, the threshold in selection proportion and hyper-parameter controlling the sparsity are calibrated by maximising the stability score. The PC-specific calibrated pair of parameters are reported in:

```{r}
stab$summary
```


### Outputs

The sets of stably selected variables for each of the PCs are encoded in:

```{r}
SelectedVariables(stab)
```

The calibrated selection proportions for each of the three PCs can be obtained from:

```{r}
SelectionProportions(stab)
```

