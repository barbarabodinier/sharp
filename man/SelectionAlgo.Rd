% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/algorithms.R
\name{SelectionAlgo}
\alias{SelectionAlgo}
\title{Variable selection algorithm}
\usage{
SelectionAlgo(x, y, lambda, family, implementation = "glmnet", ...)
}
\arguments{
\item{x}{matrix of predictors with observations as rows and variables as
columns.}

\item{y}{vector or matrix of outcome(s).}

\item{lambda}{matrix of parameters controlling the underlying feature
selection algorithm specified in "implementation". With
implementation="glmnet", these are penalty parameters controlling the
regularised model.}

\item{family}{type of regression model. This argument is defined as in the
\code{\link[glmnet]{glmnet}} function from the glmnet package. Possible values
include "gaussian" (linear regression), "binomial" (logistic regression),
"multinomial" (multinomial regression), and "cox" (survival analysis). This
argument is only used with implementation="glmnet", or with functions using
the family argument in the same way (see example below).}

\item{implementation}{name of the function to use for variable selection.
With implementation="glmnet", the function \code{\link[glmnet]{glmnet}} is called.
Alternatively, this argument can be a character string indicating the name
of a function. The function provided must use arguments called "x", "y",
"lambda" and "family" and return matrices of model coefficients (see
example below).}

\item{...}{additional parameters passed to the function provided in
"implementation".}
}
\value{
A list with: \item{selected}{matrix of binary selection status. Rows
  correspond to different model parameters. Columns correspond to
  predictors.} \item{beta_full}{array of model coefficients. Rows correspond
  to different model parameters. Columns correspond to predictors. Indices
  along the third dimension correspond to outcome variable(s).}
}
\description{
Runs the variable selection algorithm specified in the argument
"implementation" and returns matrices of model coefficients. This function is
not using stability.
}
\examples{
# Data simulation
set.seed(1)
simul=SimulateRegression(pk=50)

# Running the LASSO
mylasso=SelectionAlgo(x=simul$X, y=simul$Y, lambda=c(0.1,0.2), family="gaussian")

# Simulation of additional outcomes
set.seed(2)
Y=cbind(simul$Y, matrix(rnorm(nrow(simul$Y)*2),ncol=2))

# Running multivariate Gaussian LASSO
mylasso=SelectionAlgo(x=simul$X, y=Y, lambda=c(0.1,0.2), family="mgaussian")
str(mylasso)

stab=VariableSelection(xdata=simul$X, ydata=Y, family="mgaussian")
SelectionPerformance(SelectedVariables(stab), simul$theta)

}
