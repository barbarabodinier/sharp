% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/explanatory_performance.R
\name{ExplanatoryPerformance}
\alias{ExplanatoryPerformance}
\title{Prediction performance in regression}
\usage{
ExplanatoryPerformance(
  xdata,
  ydata,
  stability = NULL,
  family = NULL,
  K = 1,
  tau = 0.8,
  seed = 1,
  n_thr = NULL,
  ij_method = FALSE,
  time = 1000
)
}
\arguments{
\item{xdata}{matrix of predictors with observations as rows and variables as
columns.}

\item{ydata}{optional vector or matrix of outcome(s). If \code{family} is set
to \code{"binomial"} or \code{"multinomial"}, \code{ydata} can be a vector
with character/numeric values, or a factor.}

\item{stability}{output of \code{\link{VariableSelection}}. With
\code{stability=NULL} (the default), a model including all variables in
\code{xdata} as predictors is fitted. Argument \code{family} must be
provided in this case.}

\item{family}{type of regression model. Possible values include
\code{"gaussian"} (linear regression), \code{"binomial"} (logistic
regression), \code{"multinomial"} (multinomial regression), and
\code{"cox"} (survival analysis). This argument must be consistent with
input \code{stability}, if provided.}

\item{K}{number of subsampling iterations.}

\item{tau}{proportion of observations used in the training set.}

\item{seed}{value of the seed to ensure reproducibility of the results.}

\item{n_thr}{number of thresholds to use to construct the ROC curve. For
faster computations on large data, values below \code{length(x)-1} can be
used. Only used for logistic regression.}

\item{ij_method}{logical indicating if the analysis should be done for only
one recalibration/test split with variance of the concordance index should
be computed using the infinitesimal jackknife method as implemented in
\code{\link[survival]{concordance}}. With \code{ij_method=TRUE}, the
concordance index and estimated confidence interval at level 0.05 are
reported. With \code{ij_method=FALSE} (the default), the concordance
indices computed for different recalibration/test splits are reported.}

\item{time}{numeric indicating the time for which the survival probabilities
are computed. Only used for Cox regression.}
}
\value{
A list with: \item{TPR}{True Positive Rate (for logistic regression
  only).} \item{FPR}{False Positive Rate (for logistic regression only).}
  \item{AUC}{Area Under the Curve (for logistic regression only).}
  \item{concordance}{Concordance index (for Cox regression only).}
  \item{lower}{lower bound of the confidence interval at level 0.05 for the
  concordance index calculated using the infinitesimal jackknife (for Cox
  regression and with \code{ij_method=TRUE}).} \item{upper}{upper bound of
  the confidence interval at level 0.05 for the concordance index calculated
  using the infinitesimal jackknife (for Cox regression and with
  \code{ij_method=TRUE}).} \item{Beta}{matrix of estimated beta coefficients
  across the \code{K} iterations. Coefficients are extracted using the
  \code{\link[stats]{coef}} function.}
}
\description{
Calculates model performance for linear (measured by Q-squared), logistic
(AUC) or Cox (C-statistic) regression. This is done by (i) recalibrating the
model on a training set including a proportion \code{tau} of the
observations, and (ii) evaluating the performance on the remaining
observations (test set). For more reliable results, the procedure can be
repeated \code{K} times (default \code{K=1}).
}
\details{
For a fair evaluation of the prediction performance, the data is
  split into a training set (including a proportion \code{tau} of the
  observations) and test set (remaining observations). The regression model
  is fitted on the training set and applied on the test set. Performance
  metrics are computed in the test set by comparing predicted and observed
  outcomes.

  For logistic regression, a Receiver Operating Characteristic (ROC) analysis
  is performed: the True and False Positive Rates (TPR and FPR) are computed
  for different thresholds in predicted probabilities.

  For Cox regression, the Concordance Index (as implemented in
  \code{\link[survival]{concordance}}) looking at survival probabilities up
  to a specific \code{time} is computed.
}
\examples{
\dontrun{
## Logistic regression

# Data simulation
set.seed(1)
simul <- SimulateRegression(n = 1000, pk = 10, family = "binomial")

# Balanced split: 50\% variable selection set and 50\% for evaluation of performances
ids_train <- Resample(
  data = simul$ydata,
  tau = 0.5, family = "binomial"
)
xtrain <- simul$xdata[ids_train, ]
ytrain <- simul$ydata[ids_train, ]
xtest <- simul$xdata[-ids_train, ]
ytest <- simul$ydata[-ids_train, ]

# Stability selection
stab <- VariableSelection(xdata = xtrain, ydata = ytrain, family = "binomial")

# Evaluation of the performances on recalibrated models (K=1)
roc <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  stability = stab, n_thr = NULL
)
PlotROC(roc)

# Using more recalibration/test splits
roc <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  stability = stab, K = 100
)
boxplot(roc$AUC, ylab = "AUC")
PlotROC(roc)

# Comparison with saturated model
roc <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  family = "binomial", K = 100
)
PlotROC(roc, col = "blue", col_band = "blue", add = TRUE)


## Cox regression

# Data simulation
set.seed(1)
simul <- SimulateRegression(n = 500, pk = 50, family = "binomial")
ydata <- cbind(
  time = runif(nrow(simul$ydata), min = 100, max = 2000),
  case = simul$ydata[, 1]
) # including dummy time to event

# Balanced split: 50\% variable selection set and 50\% for evaluation of performances
ids_train <- Resample(
  data = simul$ydata,
  tau = 0.5, family = "binomial"
)
xtrain <- simul$xdata[ids_train, ]
ytrain <- ydata[ids_train, ]
xtest <- simul$xdata[-ids_train, ]
ytest <- ydata[-ids_train, ]

# Stability selection
stab <- VariableSelection(xdata = xtrain, ydata = ytrain, family = "cox")

# Evaluation of the performances on recalibrated models (K=1)
perf <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  stability = stab, ij_method = TRUE
)
print(perf)

# Using more recalibration/test splits
perf <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  stability = stab, K = 10, time = 1000
)
boxplot(perf$concordance)


## Linear regression

# Data simulation
set.seed(1)
simul <- SimulateRegression(n = 1000, pk = 10, family = "gaussian")

# Balanced split: 50\% variable selection set and 50\% for evaluation of performances
ids_train <- Resample(
  data = simul$ydata,
  tau = 0.5, family = "gaussian"
)
xtrain <- simul$xdata[ids_train, ]
ytrain <- simul$ydata[ids_train, ]
xtest <- simul$xdata[-ids_train, ]
ytest <- simul$ydata[-ids_train, ]

# Stability selection
stab <- VariableSelection(xdata = xtrain, ydata = ytrain, family = "gaussian")

# Evaluation of the performances on recalibrated models (K=1)
perf <- ExplanatoryPerformance(
  xdata = xtest, ydata = ytest,
  stability = stab
)
}

}
\seealso{
\code{\link{VariableSelection}}, \code{\link{Recalibrate}}

Other prediction performance functions: 
\code{\link{Incremental}()},
\code{\link{PlotIncremental}()},
\code{\link{PlotROC}()},
\code{\link{ROC}()}
}
\concept{prediction performance functions}
