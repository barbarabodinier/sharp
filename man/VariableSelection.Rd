% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/variable_selection.R
\name{VariableSelection}
\alias{VariableSelection}
\title{Stability selection in regression}
\usage{
VariableSelection(
  xdata,
  ydata = NULL,
  Lambda = NULL,
  pi_list = seq(0.6, 0.9, by = 0.01),
  K = 100,
  tau = 0.5,
  seed = 1,
  n_cat = 3,
  family = "gaussian",
  implementation = PenalisedRegression,
  resampling = "subsampling",
  PFER_method = "MB",
  PFER_thr = Inf,
  FDP_thr = Inf,
  Lambda_cardinal = 100,
  group_x = NULL,
  group_penalisation = FALSE,
  n_cores = 1,
  output_data = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{xdata}{matrix of predictors with observations as rows and variables as
columns.}

\item{ydata}{optional vector or matrix of outcome(s). If \code{family} is set
to \code{"binomial"} or \code{"multinomial"}, \code{ydata} can be a vector
with character/numeric values, or a factor.}

\item{Lambda}{matrix of parameters controlling the level of sparsity in the
underlying feature selection algorithm specified in \code{implementation}.
With \code{implementation=PenalisedRegression}, \code{Lambda} contains
penalty parameters. If \code{Lambda=NULL},
\code{\link{LambdaGridRegression}} is used to define a relevant grid. If
\code{implementation} is not set to \code{PenalisedRegression},
\code{Lambda} must be provided.}

\item{pi_list}{vector of thresholds in selection proportions. If
\code{n_cat=3}, these values must be \code{>0.5} and \code{<1}. If
\code{n_cat=2}, these values must be \code{>0} and \code{<1}.}

\item{K}{number of resampling iterations.}

\item{tau}{subsample size. Only used with \code{resampling="subsampling"}.}

\item{seed}{value of the seed to ensure reproducibility of the results.}

\item{n_cat}{number of categories used to compute the stability score.
Possible values are 2 or 3.}

\item{family}{type of regression model. If
\code{implementation=PenalisedRegression}, this argument is defined as in
\code{\link[glmnet]{glmnet}}. Possible values include \code{"gaussian"}
(linear regression), \code{"binomial"} (logistic regression),
\code{"multinomial"} (multinomial regression), and \code{"cox"} (survival
analysis).}

\item{implementation}{function to use for variable selection. By default,
\code{PenalisedRegression}, based on \code{\link[glmnet]{glmnet}}, is used
for regularised regression. Other possible functions are: \code{SparsePLS},
\code{GroupPLS} and \code{SparseGroupPLS}. Alternatively, a function with
arguments \code{xdata}, \code{ydata}, \code{Lambda}, \code{family} and
\code{...}, and returning a list of two matrices named \code{selected} and
\code{beta_full} of the correct dimensions can be used.}

\item{resampling}{resampling approach. Possible values are:
\code{"subsampling"} for sampling without replacement of a proportion
\code{tau} of the observations, or \code{"bootstrap"} for sampling with
replacement generating a resampled dataset with as many observations as in
the full sample. Alternatively, this argument can be a function to use for
resampling. This function must use arguments named \code{data} and
\code{tau} and return IDs of observations to be included in the resampled
dataset (see example in \code{\link{Resample}}).}

\item{PFER_method}{method used to compute the upper-bound of the expected
number of False Positives (or Per Family Error Rate, PFER). With
\code{PFER_method="MB"}, the method proposed by Meinshausen and BÃ¼hlmann
(2010) is used. With \code{PFER_method="SS"}, the method proposed by Shah
and Samworth (2013) under the assumption of unimodality is used.}

\item{PFER_thr}{threshold in PFER for constrained calibration by error
control. With \code{PFER_thr=Inf} and \code{FDP_thr=Inf}, unconstrained
calibration is used.}

\item{FDP_thr}{threshold in the expected proportion of falsely selected
features (or False Discovery Proportion, FDP) for constrained calibration
by error control. With \code{PFER_thr=Inf} and \code{FDP_thr=Inf},
unconstrained calibration is used.}

\item{Lambda_cardinal}{number of values in the grid of parameters controlling
the level of sparsity in the underlying algorithm.}

\item{group_x}{vector encoding the grouping structure among predictors. This
argument indicates the number of variables in each group. Only used with
\code{implementation=SparseGroupPLS} or \code{implementation=GroupPLS}.}

\item{group_penalisation}{logical indicating if a group penalisation should
be considered in the stability score. An extra argument \code{group_x}, a
vector encoding the number of variables in each group, must be provided if
\code{group_penalisation=TRUE}. The use of \code{group_penalisation=TRUE}
strictly applies to group (not sparse-group) penalisation.}

\item{n_cores}{number of cores to use for parallel computing. Only available
on Unix systems.}

\item{output_data}{logical indicating if the input datasets \code{xdata} and
\code{ydata} should be included in the output.}

\item{verbose}{logical indicating if a loading bar and messages should be
printed.}

\item{...}{additional parameters passed to the functions provided in
\code{implementation} or \code{resampling}.}
}
\value{
A list with: \item{S}{a matrix of the best stability scores for
  different parameters controlling the level of sparsity in the underlying
  algorithm.} \item{Lambda}{a matrix of parameters controlling the level of
  sparsity in the underlying algorithm.} \item{Q}{a matrix of the average
  number of selected features by underlying algorithm with different
  parameters controlling the level of sparsity.} \item{Q_s}{a matrix of the
  calibrated number of stably selected features with different parameters
  controlling the level of sparsity.} \item{P}{a matrix of calibrated
  thresholds in selection proportions for different parameters controlling
  the level of sparsity in the underlying algorithm.} \item{PFER}{a matrix of
  the upper-bounds in PFER of calibrated stability selection models with
  different parameters controlling the level of sparsity.} \item{FDP}{a
  matrix of the upper-bounds in FDP of calibrated stability selection models
  with different parameters controlling the level of sparsity.} \item{S_2d}{a
  matrix of stability scores obtained with different combinations of
  parameters. Columns correspond to different tresholds in selection
  proportions.} \item{selprop}{a matrix of selection proportions. Columns
  correspond to predictors from \code{xdata}.} \item{Beta}{an array of model
  coefficients. Columns correspond to predictors from \code{xdata}. Indices
  along the third dimension correspond to different resampling iterations.
  With multivariate outcomes, indices along the fourth dimension correspond
  to outcome-specific coefficients.} \item{method}{a list with
  \code{type="variable_selection"}, \code{implementation}, \code{family},
  \code{resampling} and \code{PFER_method} values used for the run.}
  \item{params}{a list of \code{K}, \code{pi_list}, \code{tau}, \code{n_cat},
  \code{pk}, \code{n} (number of observations), \code{PFER_thr},
  \code{FDP_thr} and \code{seed} values used for the run. The datasets
  \code{xdata} and \code{ydata} are also included if
  \code{output_data=TRUE}.} For all objects except those stored in
  \code{methods} or \code{params}, rows correspond to parameter values stored
  in the output \code{Lambda}.
}
\description{
Runs stability selection regression models with different combinations of
parameters controlling the sparsity of the underlying selection algorithm
(e.g. penalty parameter for regularised models) and thresholds in selection
proportions. These two parameters are jointly calibrated by maximising the
stability score of the model (possibly under a constraint on the expected
number of falsely stably selected features).
}
\details{
To ensure reproducibility of the results, the state of the random
  number generator is fixed to \code{seed}. For parallelisation of the code,
  stability selection results produced with different \code{seed}s and all
  other parameters equal can be combined (more details in
  \code{\link{Combine}}).
}
\examples{
\dontshow{
# Linear regression
set.seed(1)
simul <- SimulateRegression(n = 50, pk = 10, family = "gaussian")
stab <- VariableSelection(xdata = simul$xdata, ydata = simul$ydata, family = "gaussian", K = 5, verbose = FALSE)
CalibrationPlot(stab)
myselected <- SelectedVariables(stab)
coefs <- Coefficients(stab)
perf <- SelectionPerformance(theta = myselected, theta_star = simul$theta[, 1])
SelectionProportions(stab)
}
\dontrun{

# Linear regression
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
stab <- VariableSelection(xdata = simul$xdata, ydata = simul$ydata, family = "gaussian")
summary(stab)
print(SelectedVariables(stab))

# Using additional arguments from glmnet (e.g. penalty.factor)
stab <- VariableSelection(
  xdata = simul$xdata, ydata = simul$ydata, family = "gaussian",
  penalty.factor = c(rep(1, 45), rep(0, 5))
)
print(SelectedVariables(stab))

# Regression with multivariate outcomes
set.seed(1)
simul <- SimulateRegression(n = 100, pk = c(20, 30), family = "gaussian")
stab <- VariableSelection(xdata = simul$xdata, ydata = simul$ydata, family = "mgaussian")
print(SelectedVariables(stab))
dim(stab$Beta)

# Logistic regression
set.seed(1)
simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
stab <- VariableSelection(xdata = simul$xdata, ydata = simul$ydata, family = "binomial")
print(SelectedVariables(stab))

# Multinomial regression
set.seed(1)
simul <- SimulateRegression(n = 200, pk = 15, family = "multinomial")
stab <- VariableSelection(
  xdata = simul$xdata, ydata = simul$ydata,
  family = "multinomial"
)
print(SelectedVariables(stab))

# Sparse PCA (1 component)
set.seed(1)
simul <- SimulateComponents(pk = c(5, 3, 4))
stab <- VariableSelection(
  xdata = simul$data,
  Lambda = 1:(ncol(simul$data) - 1),
  implementation = SparsePCA
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Sparse PLS (1 outcome, 1 component)
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
stab <- VariableSelection(
  xdata = simul$xdata, ydata = simul$ydata,
  Lambda = 1:(ncol(simul$xdata) - 1),
  implementation = SparsePLS, family = "gaussian"
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Group PLS (1 outcome, 1 component)
stab <- VariableSelection(
  xdata = simul$xdata, ydata = simul$ydata,
  Lambda = 1:5,
  group_x = c(5, 5, 10, 20, 10),
  group_penalisation = TRUE,
  implementation = GroupPLS, family = "gaussian"
)
par(mar = c(3, 5, 1, 5))
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Sparse PLS-DA (1 outcome, 1 component)
set.seed(1)
simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
stab <- VariableSelection(
  xdata = simul$xdata, ydata = simul$ydata,
  Lambda = 1:(ncol(simul$xdata) - 1),
  implementation = SparsePLS,
  family = "binomial"
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Example using an external function: group-LASSO with gglasso
if (requireNamespace("gglasso", quietly = TRUE)) {
  set.seed(1)
  simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
  ManualGridGroupLasso <- function(xdata, ydata, family, group_x, ...) {
    # Defining the grouping
    group <- do.call(c, lapply(1:length(group_x), FUN = function(i) {
      rep(i, group_x[i])
    }))

    if (family == "binomial") {
      ytmp <- ydata
      ytmp[ytmp == min(ytmp)] <- -1
      ytmp[ytmp == max(ytmp)] <- 1
      return(gglasso::gglasso(xdata, ytmp, loss = "logit", group = group, ...))
    } else {
      return(gglasso::gglasso(xdata, ydata, lambda = lambda, loss = "ls", group = group, ...))
    }
  }
  Lambda <- LambdaGridRegression(
    xdata = simul$xdata, ydata = simul$ydata,
    family = "binomial", Lambda_cardinal = 20,
    implementation = ManualGridGroupLasso,
    group_x = rep(5, 4)
  )
  GroupLasso <- function(xdata, ydata, Lambda, family, group_x, ...) {
    # Defining the grouping
    group <- do.call(c, lapply(1:length(group_x), FUN = function(i) {
      rep(i, group_x[i])
    }))

    # Running the regression
    if (family == "binomial") {
      ytmp <- ydata
      ytmp[ytmp == min(ytmp)] <- -1
      ytmp[ytmp == max(ytmp)] <- 1
      mymodel <- gglasso::gglasso(xdata, ytmp, lambda = Lambda, loss = "logit", group = group, ...)
    }
    if (family == "gaussian") {
      mymodel <- gglasso::gglasso(xdata, ydata, lambda = Lambda, loss = "ls", group = group, ...)
    }
    # Extracting and formatting the beta coefficients
    beta_full <- t(as.matrix(mymodel$beta))
    beta_full <- beta_full[, colnames(xdata)]

    selected <- ifelse(beta_full != 0, yes = 1, no = 0)

    return(list(selected = selected, beta_full = beta_full))
  }
  stab <- VariableSelection(
    xdata = simul$xdata, ydata = simul$ydata,
    implementation = GroupLasso, family = "binomial", Lambda = Lambda,
    group_x = rep(5, 4),
    group_penalisation = TRUE
  )
  print(SelectedVariables(stab))
}
}
}
\references{
\insertRef{ourstabilityselection}{focus}

  \insertRef{stabilityselectionMB}{focus}

  \insertRef{stabilityselectionSS}{focus}
}
\seealso{
\code{\link{Recalibrate}}, \code{\link{ExplanatoryPerformance}},
  \code{\link{PlotROC}}, \code{\link{Incremental}},
  \code{\link{PlotIncremental}}, \code{\link{Combine}},
  \code{\link{LambdaGridRegression}}, \code{\link{Resample}},
  \code{\link{StabilityScore}}

Other stability selection functions: 
\code{\link{BiSelection}()},
\code{\link{Clustering}()},
\code{\link{GraphicalModel}()}
}
\concept{stability selection functions}
