% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/variable_selection.R
\name{VariableSelection}
\alias{VariableSelection}
\title{Stability selection in regression}
\usage{
VariableSelection(
  xdata,
  ydata = NULL,
  Lambda = NULL,
  pi_list = seq(0.6, 0.9, by = 0.01),
  K = 100,
  tau = 0.5,
  seed = 1,
  n_cat = 3,
  family = "gaussian",
  implementation = "glmnet",
  resampling = "subsampling",
  PFER_method = "MB",
  PFER_thr = Inf,
  FDP_thr = Inf,
  Lambda_cardinal = 100,
  n_cores = 1,
  output_data = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{xdata}{matrix of predictors with observations as rows and variables as
columns.}

\item{ydata}{vector or matrix of outcome(s).}

\item{Lambda}{matrix of parameters controlling the level of sparsity in the
underlying feature selection algorithm specified in \code{implementation}.
With \code{implementation="glmnet"}, \code{Lambda} contains penalty
parameters. If \code{Lambda=NULL}, \code{\link{LambdaGridRegression}} is
used to define a relevant grid. If \code{implementation} is not set to
\code{"glmnet"}, \code{Lambda} must be provided.}

\item{pi_list}{vector of thresholds in selection proportions. If
\code{n_cat=3}, these values must be \code{>0.5} and \code{<1}. If
\code{n_cat=2}, these values must be \code{>0} and \code{<1}.}

\item{K}{number of resampling iterations.}

\item{tau}{subsample size. Only used with \code{resampling="subsampling"}.}

\item{seed}{value of the seed.}

\item{n_cat}{number of categories used to compute the stability score.
Possible values are 2 or 3.}

\item{family}{type of regression model. If \code{implementation="glmnet"},
this argument is defined as in \code{\link[glmnet]{glmnet}}. Possible
values include \code{"gaussian"} (linear regression), \code{"binomial"}
(logistic regression), \code{"multinomial"} (multinomial regression), and
\code{"cox"} (survival analysis).}

\item{implementation}{character string indicating the name of the function to
use for variable selection. If \code{implementation="glmnet"},
\code{\link[glmnet]{glmnet}} is used for regularised regression.
Alternatively, a function with arguments \code{x}, \code{y}, \code{lambda},
\code{family} and \code{...}, and returning a list of two matrices named
\code{selected} and \code{beta_full} of the correct dimensions can be used
(more details in \code{\link{SelectionAlgo}}).}

\item{resampling}{resampling approach. Possible values are:
\code{"subsampling"} for sampling without replacement of a proportion
\code{tau} of the observations, or \code{"bootstrap"} for sampling with
replacement generating a resampled dataset with as many observations as in
the full sample. Alternatively, this argument can be a character string
indicating the name of a function to use for resampling. This function must
use arguments named \code{"data"} and \code{"tau"} and return IDs of
observations to be included in the resampled dataset (see example in
\code{\link{Resample}}).}

\item{PFER_method}{method used to compute the upper-bound of the expected
number of False Positives (or Per Family Error Rate, PFER). With
\code{PFER_method="MB"}, the method proposed by Meinshausen and BÃ¼hlmann
(2010) is used. With \code{PFER_method="SS"}, the method proposed by Shah
and Samworth (2013) under the assumption of unimodality is used.}

\item{PFER_thr}{threshold in PFER for constrained calibration by error
control. With \code{PFER_thr=Inf} and \code{FDP_thr=Inf}, unconstrained
calibration is used.}

\item{FDP_thr}{threshold in the expected proportion of falsely selected
features (or False Discovery Proportion, FDP) for constrained calibration
by error control. With \code{PFER_thr=Inf} and \code{FDP_thr=Inf},
unconstrained calibration is used.}

\item{Lambda_cardinal}{number of values in the grid of parameters controlling
the level of sparsity in the underlying algorithm.}

\item{n_cores}{number of cores to use for parallel computing. Only available
on Unix systems.}

\item{output_data}{logical indicating if the input datasets \code{xdata} and
\code{ydata} should be included in the output.}

\item{verbose}{logical indicating if a loading bar and messages should be
printed.}

\item{...}{additional parameters passed to the functions provided in
\code{"implementation"} or \code{"resampling"}.}
}
\value{
A list with: \item{S}{a matrix of the best stability scores for
  different parameters controlling the level of sparsity in the underlying
  algorithm.} \item{Lambda}{a matrix of parameters controlling the level of
  sparsity in the underlying algorithm.} \item{Q}{a matrix of the average
  number of selected features by underlying algorithm with different
  parameters controlling the level of sparsity.} \item{Q_s}{a matrix of the
  calibrated number of stably selected features with different parameters
  controlling the level of sparsity.} \item{P}{a matrix of calibrated
  thresholds in selection proportions for different parameters controlling
  the level of sparsity in the underlying algorithm.} \item{PFER}{a matrix of
  the upper-bounds in PFER of calibrated stability selection models with
  different parameters controlling the level of sparsity.} \item{FDP}{a
  matrix of the upper-bounds in FDP of calibrated stability selection models
  with different parameters controlling the level of sparsity.} \item{S_2d}{a
  matrix of stability scores obtained with different combinations of
  parameters. Columns correspond to different tresholds in selection
  proportions.} \item{selprop}{a matrix of selection proportions. Columns
  correspond to predictors from \code{xdata}.} \item{Beta}{an array of model
  coefficients. Columns correspond to predictors from \code{xdata}. Indices
  along the third dimension correspond to different resampling iterations.
  With multivariate outcomes, indices along the fourth dimension correspond
  to outcome-specific coefficients.} \item{method}{a list of
  \code{implementation}, \code{family}, \code{resampling} and
  \code{PFER_method} values used for the run.} \item{param}{a list of
  \code{K}, \code{pi_list}, \code{tau}, \code{n_cat}, \code{pk}, \code{n}
  (number of observations), \code{PFER_thr}, \code{FDP_thr} and \code{seed}
  values used for the run. The datasets \code{xdata} and \code{ydata} are
  also included if \code{output_data=TRUE}.} For all objects except those
  stored in \code{methods} or \code{params}, rows correspond to parameter
  values stored in the output \code{Lambda}.
}
\description{
Runs stability selection regression models with different combinations of
parameters controlling the sparsity of the underlying selection algorithm
(e.g. penalty parameter for regularised models) and thresholds in selection
proportions. These two parameters are jointly calibrated by maximising the
stability score of the model (possibly under a constraint on the expected
number of falsely stably selected features).
}
\details{
To ensure reproducibility of the results, the state of the random
  number generator is fixed to \code{seed}. For parallelisation of the code,
  stability selection results produced with different \code{seed}s and all
  other parameters equal can be combined (more details in
  \code{\link{Combine}}).
}
\examples{
\dontshow{
# Linear regression
set.seed(1)
simul <- SimulateRegression(n = 50, pk = 10, family = "gaussian")
stab <- VariableSelection(xdata = simul$X, ydata = simul$Y, family = "gaussian", K = 5, verbose = FALSE)
CalibrationPlot(stab)
myselected <- SelectedVariables(stab)
coefs <- Coefficients(stab)
perf <- SelectionPerformance(theta = myselected, theta_star = simul$theta)
}
\dontrun{

# Linear regression
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
stab <- VariableSelection(xdata = simul$X, ydata = simul$Y, family = "gaussian")
print(SelectedVariables(stab))

# Regression with multivariate outcomes
set.seed(2)
Y <- cbind(simul$Y, matrix(rnorm(nrow(simul$Y) * 2), ncol = 2))
stab <- VariableSelection(xdata = simul$X, ydata = Y, family = "mgaussian")
print(SelectedVariables(stab))
dim(stab$Beta)

# Logistic regression
set.seed(1)
simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
stab <- VariableSelection(xdata = simul$X, ydata = simul$Y, family = "binomial")
print(SelectedVariables(stab))

# Multinomial regression
set.seed(2)
Y <- simul$Y + sample(c(0, 1), size = nrow(simul$Y), replace = TRUE)
stab <- VariableSelection(
  xdata = simul$X, ydata = Y,
  family = "multinomial", lambda.min.ratio = 0.1
)
print(SelectedVariables(stab))

# Sparse PLS (1 outcome)
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
stab <- VariableSelection(
  xdata = simul$X, ydata = simul$Y,
  Lambda = 1:(ncol(simul$X) - 1),
  implementation = "SparsePLS", family = "gaussian"
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Sparse PLS (sparse on X, 1 component)
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
ydata <- cbind(simul$Y, matrix(rnorm(100 * 3), ncol = 3))
colnames(ydata) <- paste0("outcome", 1:4)
x <- simul$X
y <- ydata
stab <- VariableSelection(
  xdata = x, ydata = y,
  Lambda = 1:(ncol(simul$X) - 1), ncomp = 1,
  implementation = "SparsePLS", family = "gaussian"
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Sparse PLS (sparse on X, multiple components)
nvar_x <- selected <- stability_score <- NULL
for (comp in 1:ncol(y)) {
  print(comp)
  stab <- VariableSelection(
    xdata = x, ydata = y, K = 10,
    Lambda = 1:(ncol(simul$X) - 1), ncomp = comp,
    keepX_previous = nvar_x,
    implementation = "SparsePLS", family = "gaussian"
  )
  nvar_x <- c(nvar_x, Argmax(stab)[1])
  selected <- rbind(selected, SelectedVariables(stab))
  stability_score <- c(stability_score, max(stab$S, na.rm = TRUE))
}
print(selected[1:which.max(stability_score), ]) # selected in calibrated model

# Sparse PLS (sparse on X and Y, multiple components)
K <- 10
nvar_x <- nvar_y <- selected_x <- selected_y <- stability_score <- NULL
for (comp in 1:ncol(y)) {
  print(comp)
  tmp_stability_score <- tmp_tokeepx <- NULL
  tmp_selected_x <- tmp_selected_y <- NULL
  for (ny in 1:ncol(y)) {
    stab <- VariableSelection(
      xdata = x, ydata = y, K = K,
      Lambda = 1:(ncol(simul$X) - 1), ncomp = comp,
      keepX_previous = nvar_x, keepY = c(nvar_y, ny),
      implementation = "SparsePLS", family = "gaussian"
    )
    selprop <- apply(Coefficients(stab, side = "Y")[ArgmaxId(stab)[1], , ], 1,
      FUN = function(z) {
        sum(z != 0) / length(z)
      }
    )
    if (any(selprop != 1)) {
      hat_pi <- stab$params$pi_list[which.max(StabilityScore(selprop,
        pi = stab$params$pi_list, K = K
      ))]
      tmp_selected_y <- rbind(
        tmp_selected_y,
        ifelse(selprop >= hat_pi, yes = 1, no = 0)
      )
    } else {
      tmp_selected_y <- rbind(tmp_selected_y, rep(1, ncol(y)))
    }
    tmp_selected_x <- rbind(tmp_selected_x, SelectedVariables(stab))
    tmp_tokeepx <- c(tmp_tokeepx, Argmax(stab)[1])
    tmp_stability_score <- c(tmp_stability_score, max(stab$S, na.rm = TRUE))
  }
  tokeepx <- tmp_tokeepx[which.max(tmp_stability_score)]
  tokeepy <- which.max(tmp_stability_score)
  nvar_x <- c(nvar_x, tokeepx)
  nvar_y <- c(nvar_y, tokeepy)
  selected_x <- rbind(selected_x, tmp_selected_x[which.max(tmp_stability_score), ])
  selected_y <- rbind(selected_y, tmp_selected_y[which.max(tmp_stability_score), ])
  stability_score <- c(stability_score, max(stab$S, na.rm = TRUE))
}
print(selected_x[1:which.max(stability_score), ]) # selected in X in calibrated model
print(selected_y[1:which.max(stability_score), ]) # selected in Y in calibrated model

# Sparse PLS-DA (1 outcome)
set.seed(1)
simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
stab <- VariableSelection(
  xdata = simul$X, ydata = simul$Y,
  Lambda = 1:(ncol(simul$X) - 1),
  implementation = "SparsePLS",
  family = "binomial"
)
CalibrationPlot(stab, xlab = "")
print(SelectedVariables(stab))

# Sparse group PLS (1 outcome)
set.seed(1)
simul <- SimulateRegression(n = 100, pk = 50, family = "gaussian")
ydata <- cbind(simul$Y, matrix(rnorm(100 * 3), ncol = 3))
colnames(ydata) <- paste0("outcome", 1:4)
x <- simul$X
y <- ydata
alpha_list <- seq(0.1, 0.9, by = 0.1)
selected <- stability_score <- NULL
for (alpha in alpha_list) {
  stab <- VariableSelection(
    xdata = x, ydata = y, K = 10,
    group_x = c(10, 10, 30), alpha.x = alpha,
    Lambda = 1:3,
    implementation = "SparseGroupPLS", family = "gaussian"
  )
  selected <- rbind(selected, SelectedVariables(stab))
  stability_score <- c(stability_score, max(stab$S, na.rm = TRUE))
}
print(selected[which.max(stability_score), ])

# Example using an external function: group-LASSO with gglasso
if (requireNamespace("gglasso", quietly = TRUE)) {
  set.seed(1)
  simul <- SimulateRegression(n = 200, pk = 20, family = "binomial")
  stab <- VariableSelection(xdata = simul$X, ydata = simul$Y, family = "binomial")
  print(SelectedVariables(stab))
  ManualGridGroupLasso <- function(x, y, family, ...) {
    if (family == "binomial") {
      ytmp <- y
      ytmp[ytmp == min(ytmp)] <- -1
      ytmp[ytmp == max(ytmp)] <- 1
      return(gglasso::gglasso(x, ytmp, loss = "logit", ...))
    } else {
      return(gglasso::gglasso(x, y, lambda = lambda, loss = "ls", ...))
    }
  }
  Lambda <- LambdaGridRegression(
    xdata = simul$X, ydata = simul$Y,
    family = "binomial", Lambda_cardinal = 20,
    implementation = "ManualGridGroupLasso",
    group = sort(rep(1:4, length.out = ncol(simul$X)))
  )
  GroupLasso <- function(x, y, lambda, family, ...) {
    # Running the regression
    if (family == "binomial") {
      ytmp <- y
      ytmp[ytmp == min(ytmp)] <- -1
      ytmp[ytmp == max(ytmp)] <- 1
      mymodel <- gglasso::gglasso(x, ytmp, lambda = lambda, loss = "logit", ...)
    }
    if (family == "gaussian") {
      mymodel <- gglasso::gglasso(x, y, lambda = lambda, loss = "ls", ...)
    }
    # Extracting and formatting the beta coefficients
    beta_full <- t(as.matrix(mymodel$beta))
    beta_full <- beta_full[, colnames(x)]

    selected <- ifelse(beta_full != 0, yes = 1, no = 0)

    return(list(selected = selected, beta_full = beta_full))
  }
  stab <- VariableSelection(
    xdata = simul$X, ydata = simul$Y,
    implementation = "GroupLasso", family = "binomial", Lambda = Lambda,
    group = sort(rep(1:4, length.out = ncol(simul$X)))
  )
  print(SelectedVariables(stab))
}
}
}
\seealso{
\code{\link{LambdaGridRegression}}, \code{\link{Resample}},
  \code{\link{Combine}}

Other stability selection functions: 
\code{\link{GraphicalModel}()}
}
\concept{stability selection functions}
