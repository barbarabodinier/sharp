@misc{ourstabilityselection,
      title={Automated calibration for stability selection in penalised regression and graphical models: a multi-OMICs network application exploring the molecular response to tobacco smoking}, 
      author={Barbara Bodinier and Sarah Filippi and Therese Haugdahl Nost and Julien Chiquet and Marc Chadeau-Hyam},
      year={2021},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url = {https://arxiv.org/abs/2106.02521}
}


@article{stabilityselectionMB,
author = {Meinshausen, Nicolai and Bühlmann, Peter},
title = {Stability selection},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {72},
number = {4},
pages = {417-473},
keywords = {High dimensional data, Resampling, Stability selection, Structure estimation},
doi = {10.1111/j.1467-9868.2010.00740.x},
abstract = {Summary.  Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
year = {2010}
}


@article{stabilityselectionSS,
author = {Shah, Rajen D. and Samworth, Richard J.},
title = {Variable selection with error control: another look at stability selection},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {75},
number = {1},
pages = {55-80},
keywords = {Complementary pairs stability selection, r-concavity, Subagging, Subsampling, Variable selection},
doi = {10.1111/j.1467-9868.2011.01034.x},
abstract = {Summary.  Stability selection was recently introduced by Meinshausen and Bühlmann as a very general technique designed to improve the performance of a variable selection algorithm. It is based on aggregating the results of applying a selection procedure to subsamples of the data. We introduce a variant, called complementary pairs stability selection, and derive bounds both on the expected number of variables included by complementary pairs stability selection that have low selection probability under the original procedure, and on the expected number of high selection probability variables that are excluded. These results require no (e.g. exchangeability) assumptions on the underlying model or on the quality of the original selection procedure. Under reasonable shape restrictions, the bounds can be further tightened, yielding improved error control, and therefore increasing the applicability of the methodology.},
year = {2013}
}


@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}


@article{sparsePCASVD,
title = {Sparse principal component analysis via regularized low rank matrix approximation},
journal = {Journal of Multivariate Analysis},
volume = {99},
number = {6},
pages = {1015-1034},
year = {2008},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2007.06.007},
author = {Haipeng Shen and Jianhua Z. Huang},
keywords = {Dimension reduction, High-dimension-low-sample-size, Regularization, Singular value decomposition, Thresholding},
abstract = {Principal component analysis (PCA) is a widely used tool for data analysis and dimension reduction in applications throughout science and engineering. However, the principal components (PCs) can sometimes be difficult to interpret, because they are linear combinations of all the original variables. To facilitate interpretation, sparse PCA produces modified PCs with sparse loadings, i.e. loadings with very few non-zero elements. In this paper, we propose a new sparse PCA method, namely sparse PCA via regularized SVD (sPCA-rSVD). We use the connection of PCA with singular value decomposition (SVD) of the data matrix and extract the PCs through solving a low rank matrix approximation problem. Regularization penalties are introduced to the corresponding minimization problem to promote sparsity in PC loadings. An efficient iterative algorithm is proposed for computation. Two tuning parameter selection methods are discussed. Some theoretical results are established to justify the use of sPCA-rSVD when only the data covariance matrix is available. In addition, we give a modified definition of variance explained by the sparse PCs. The sPCA-rSVD provides a uniform treatment of both classical multivariate data and high-dimension-low-sample-size (HDLSS) data. Further understanding of sPCA-rSVD and some existing alternatives is gained through simulation studies and real data examples, which suggests that sPCA-rSVD provides competitive results.}
}


@article{sparsePCA,
author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
title = {Sparse Principal Component Analysis},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {2},
pages = {265-286},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X113430},
}


@article{PLS,
title = {PLS-regression: a basic tool of chemometrics},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {58},
number = {2},
pages = {109-130},
year = {2001},
note = {PLS Methods},
issn = {0169-7439},
doi = {https://doi.org/10.1016/S0169-7439(01)00155-1},
author = {Svante Wold and Michael Sjöström and Lennart Eriksson},
keywords = {PLS, PLSR, Two-block predictive PLS, Latent variables, Multivariate analysis},
abstract = {PLS-regression (PLSR) is the PLS approach in its simplest, and in chemistry and technology, most used form (two-block predictive PLS). PLSR is a method for relating two data matrices, X and Y, by a linear multivariate model, but goes beyond traditional regression in that it models also the structure of X and Y. PLSR derives its usefulness from its ability to analyze data with many, noisy, collinear, and even incomplete variables in both X and Y. PLSR has the desirable property that the precision of the model parameters improves with the increasing number of relevant variables and observations. This article reviews PLSR as it has developed to become a standard tool in chemometrics and used in chemistry and engineering. The underlying model and its assumptions are discussed, and commonly used diagnostics are reviewed together with the interpretation of resulting parameters. Two examples are used as illustrations: First, a Quantitative Structure–Activity Relationship (QSAR)/Quantitative Structure–Property Relationship (QSPR) data set of peptides is used to outline how to develop, interpret and refine a PLSR model. Second, a data set from the manufacturing of recycled paper is analyzed to illustrate time series modelling of process data by means of PLSR and time-lagged X-variables.}
}


@article{sparsePLS,
   author = {KA, Lê Cao and Rossouw, D. and Robert-Granié, C. and Besse, P.},
   title = {A sparse PLS for variable selection when integrating omics data},
   journal = {Stat Appl Genet Mol Biol},
   volume = {7},
   number = {1},
   pages = {Article 35},
   ISSN = {1544-6115},
   DOI = {10.2202/1544-6115.1390},
   year = {2008},
   type = {Journal Article}
}



@article{sparsegroupPLS,
   author = {Liquet, B. and de Micheaux, P. L. and Hejblum, B. P. and Thiébaut, R.},
   title = {Group and sparse group partial least square approaches applied in genomics context},
   journal = {Bioinformatics},
   volume = {32},
   number = {1},
   pages = {35-42},
   ISSN = {1367-4803},
   DOI = {10.1093/bioinformatics/btv535},
   year = {2016},
   type = {Journal Article}
}


@article{SparseClustering,
author = {Daniela M. Witten and Robert Tibshirani},
title = {A Framework for Feature Selection in Clustering},
journal = {Journal of the American Statistical Association},
volume = {105},
number = {490},
pages = {713-726},
year  = {2010},
publisher = {Taylor & Francis},
doi = {10.1198/jasa.2010.tm09415},
    note ={PMID: 20811510}
}



@article{ConsensusClustering,
	abstract = {In this paper we present a new methodology of class discovery and clustering validation tailored to the task of analyzing gene expression data. The method can best be thought of as an analysis approach, to guide and assist in the use of any of a wide range of available clustering algorithms. We call the new methodology consensus clustering, and in conjunction with resampling techniques, it provides for a method to represent the consensus across multiple runs of a clustering algorithm and to assess the stability of the discovered clusters. The method can also be used to represent the consensus over multiple runs of a clustering algorithm with random restart (such as K-means, model-based Bayesian clustering, SOM, etc.), so as to account for its sensitivity to the initial conditions. Finally, it provides for a visualization tool to inspect cluster number, membership, and boundaries. We present the results of our experiments on both simulated data and real gene expression data aimed at evaluating the effectiveness of the methodology in discovering biologically meaningful clusters.},
	author = {Monti, Stefano and Tamayo, Pablo and Mesirov, Jill and Golub, Todd},
	da = {2003/07/01},
	date-added = {2021-12-02 13:41:40 +0100},
	date-modified = {2021-12-02 13:41:40 +0100},
	doi = {10.1023/A:1023949509487},
	id = {Monti2003},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {91--118},
	title = {Consensus Clustering: A Resampling-Based Method for Class Discovery and Visualization of Gene Expression Microarray Data},
	ty = {JOUR},
	volume = {52},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1023949509487}
	}



