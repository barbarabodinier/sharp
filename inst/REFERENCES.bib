%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Barbara Bodinier at 2023-04-17 17:01:47 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@book{CART,
	author = {Breiman, L and Friedman, JH and Olshen, R and Stone, CJ},
	date-added = {2023-04-17 16:56:55 +0200},
	date-modified = {2023-04-17 17:01:42 +0200},
	publisher = {Wadsworth},
	title = {Classification and Regression Trees},
	year = {1984}}

@article{AdaptiveLasso,
	author = {Zou, Hui},
	date-added = {2023-03-29 11:34:02 +0200},
	date-modified = {2023-03-29 11:34:08 +0200},
	journal = {Journal of the American statistical association},
	number = {476},
	pages = {1418--1429},
	publisher = {Taylor \& Francis},
	title = {The adaptive lasso and its oracle properties},
	volume = {101},
	year = {2006}}

@article{GraphicalLassoTibshirani,
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date-added = {2022-10-14 14:29:55 +0200},
	date-modified = {2022-10-14 14:29:55 +0200},
	journal = {Biostatistics},
	number = {3},
	pages = {432--441},
	publisher = {Oxford University Press},
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	year = {2008}}

@book{lavaanBook,
	author = {Gana, Kamel and Broc, Guillaume},
	date-added = {2022-10-14 14:27:54 +0200},
	date-modified = {2022-10-14 14:28:04 +0200},
	publisher = {John Wiley \& Sons},
	title = {Structural equation modeling with lavaan},
	year = {2019}}

@article{RegSEM,
	author = {Jacobucci, Ross and Grimm, Kevin J and McArdle, John J},
	date-added = {2022-10-14 14:26:15 +0200},
	date-modified = {2022-10-14 14:26:15 +0200},
	doi = {10.1080/10705511.2016.1154793},
	journal = {Structural equation modeling: a multidisciplinary journal},
	number = {4},
	pages = {555--566},
	publisher = {Taylor \& Francis},
	title = {Regularized structural equation modeling},
	volume = {23},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/10705511.2016.1154793}}

@misc{ourstabilityselection,
	archiveprefix = {arXiv},
	author = {Barbara Bodinier and Sarah Filippi and Therese Haugdahl Nost and Julien Chiquet and Marc Chadeau-Hyam},
	primaryclass = {stat.ME},
	title = {Automated calibration for stability selection in penalised regression and graphical models: a multi-OMICs network application exploring the molecular response to tobacco smoking},
	url = {https://arxiv.org/abs/2106.02521},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2106.02521}}

@article{stabilityselectionMB,
	abstract = {Summary.  Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
	author = {Meinshausen, Nicolai and B{\"u}hlmann, Peter},
	doi = {10.1111/j.1467-9868.2010.00740.x},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {High dimensional data, Resampling, Stability selection, Structure estimation},
	number = {4},
	pages = {417-473},
	title = {Stability selection},
	volume = {72},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2010.00740.x}}

@article{stabilityselectionSS,
	abstract = {Summary.  Stability selection was recently introduced by Meinshausen and B{\"u}hlmann as a very general technique designed to improve the performance of a variable selection algorithm. It is based on aggregating the results of applying a selection procedure to subsamples of the data. We introduce a variant, called complementary pairs stability selection, and derive bounds both on the expected number of variables included by complementary pairs stability selection that have low selection probability under the original procedure, and on the expected number of high selection probability variables that are excluded. These results require no (e.g. exchangeability) assumptions on the underlying model or on the quality of the original selection procedure. Under reasonable shape restrictions, the bounds can be further tightened, yielding improved error control, and therefore increasing the applicability of the methodology.},
	author = {Shah, Rajen D. and Samworth, Richard J.},
	doi = {10.1111/j.1467-9868.2011.01034.x},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Complementary pairs stability selection, r-concavity, Subagging, Subsampling, Variable selection},
	number = {1},
	pages = {55-80},
	title = {Variable selection with error control: another look at stability selection},
	volume = {75},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1111/j.1467-9868.2011.01034.x}}

@article{GraphicalLasso,
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	journal = {Biostatistics},
	number = {3},
	pages = {432--441},
	publisher = {Oxford University Press},
	title = {Sparse inverse covariance estimation with the graphical lasso},
	volume = {9},
	year = {2008}}

@article{lasso,
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	author = {Robert Tibshirani},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {267--288},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Regression Shrinkage and Selection via the Lasso},
	url = {http://www.jstor.org/stable/2346178},
	volume = {58},
	year = {1996},
	bdsk-url-1 = {http://www.jstor.org/stable/2346178}}

@article{sparsePCASVD,
	abstract = {Principal component analysis (PCA) is a widely used tool for data analysis and dimension reduction in applications throughout science and engineering. However, the principal components (PCs) can sometimes be difficult to interpret, because they are linear combinations of all the original variables. To facilitate interpretation, sparse PCA produces modified PCs with sparse loadings, i.e. loadings with very few non-zero elements. In this paper, we propose a new sparse PCA method, namely sparse PCA via regularized SVD (sPCA-rSVD). We use the connection of PCA with singular value decomposition (SVD) of the data matrix and extract the PCs through solving a low rank matrix approximation problem. Regularization penalties are introduced to the corresponding minimization problem to promote sparsity in PC loadings. An efficient iterative algorithm is proposed for computation. Two tuning parameter selection methods are discussed. Some theoretical results are established to justify the use of sPCA-rSVD when only the data covariance matrix is available. In addition, we give a modified definition of variance explained by the sparse PCs. The sPCA-rSVD provides a uniform treatment of both classical multivariate data and high-dimension-low-sample-size (HDLSS) data. Further understanding of sPCA-rSVD and some existing alternatives is gained through simulation studies and real data examples, which suggests that sPCA-rSVD provides competitive results.},
	author = {Haipeng Shen and Jianhua Z. Huang},
	doi = {https://doi.org/10.1016/j.jmva.2007.06.007},
	issn = {0047-259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Dimension reduction, High-dimension-low-sample-size, Regularization, Singular value decomposition, Thresholding},
	number = {6},
	pages = {1015-1034},
	title = {Sparse principal component analysis via regularized low rank matrix approximation},
	volume = {99},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1016/j.jmva.2007.06.007}}

@article{sparsePCA,
	author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
	doi = {10.1198/106186006X113430},
	journal = {Journal of Computational and Graphical Statistics},
	number = {2},
	pages = {265-286},
	publisher = {Taylor & Francis},
	title = {Sparse Principal Component Analysis},
	volume = {15},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/106186006X113430}}

@article{PLS,
	abstract = {PLS-regression (PLSR) is the PLS approach in its simplest, and in chemistry and technology, most used form (two-block predictive PLS). PLSR is a method for relating two data matrices, X and Y, by a linear multivariate model, but goes beyond traditional regression in that it models also the structure of X and Y. PLSR derives its usefulness from its ability to analyze data with many, noisy, collinear, and even incomplete variables in both X and Y. PLSR has the desirable property that the precision of the model parameters improves with the increasing number of relevant variables and observations. This article reviews PLSR as it has developed to become a standard tool in chemometrics and used in chemistry and engineering. The underlying model and its assumptions are discussed, and commonly used diagnostics are reviewed together with the interpretation of resulting parameters. Two examples are used as illustrations: First, a Quantitative Structure--Activity Relationship (QSAR)/Quantitative Structure--Property Relationship (QSPR) data set of peptides is used to outline how to develop, interpret and refine a PLSR model. Second, a data set from the manufacturing of recycled paper is analyzed to illustrate time series modelling of process data by means of PLSR and time-lagged X-variables.},
	author = {Svante Wold and Michael Sj{\"o}str{\"o}m and Lennart Eriksson},
	doi = {https://doi.org/10.1016/S0169-7439(01)00155-1},
	issn = {0169-7439},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	keywords = {PLS, PLSR, Two-block predictive PLS, Latent variables, Multivariate analysis},
	note = {PLS Methods},
	number = {2},
	pages = {109-130},
	title = {PLS-regression: a basic tool of chemometrics},
	volume = {58},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1016/S0169-7439(01)00155-1}}

@article{sparsePLS,
	author = {KA, L{\^e} Cao and Rossouw, D. and Robert-Grani{\'e}, C. and Besse, P.},
	doi = {10.2202/1544-6115.1390},
	issn = {1544-6115},
	journal = {Stat Appl Genet Mol Biol},
	number = {1},
	pages = {Article 35},
	title = {A sparse PLS for variable selection when integrating omics data},
	type = {Journal Article},
	volume = {7},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.2202/1544-6115.1390}}

@article{sparsegroupPLS,
	author = {Liquet, B. and de Micheaux, P. L. and Hejblum, B. P. and Thi{\'e}baut, R.},
	doi = {10.1093/bioinformatics/btv535},
	issn = {1367-4803},
	journal = {Bioinformatics},
	number = {1},
	pages = {35-42},
	title = {Group and sparse group partial least square approaches applied in genomics context},
	type = {Journal Article},
	volume = {32},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1093/bioinformatics/btv535}}

@article{SparseClustering,
	author = {Daniela M. Witten and Robert Tibshirani},
	doi = {10.1198/jasa.2010.tm09415},
	journal = {Journal of the American Statistical Association},
	note = {PMID: 20811510},
	number = {490},
	pages = {713-726},
	publisher = {Taylor & Francis},
	title = {A Framework for Feature Selection in Clustering},
	volume = {105},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1198/jasa.2010.tm09415}}

@article{ConsensusClustering,
	abstract = {In this paper we present a new methodology of class discovery and clustering validation tailored to the task of analyzing gene expression data. The method can best be thought of as an analysis approach, to guide and assist in the use of any of a wide range of available clustering algorithms. We call the new methodology consensus clustering, and in conjunction with resampling techniques, it provides for a method to represent the consensus across multiple runs of a clustering algorithm and to assess the stability of the discovered clusters. The method can also be used to represent the consensus over multiple runs of a clustering algorithm with random restart (such as K-means, model-based Bayesian clustering, SOM, etc.), so as to account for its sensitivity to the initial conditions. Finally, it provides for a visualization tool to inspect cluster number, membership, and boundaries. We present the results of our experiments on both simulated data and real gene expression data aimed at evaluating the effectiveness of the methodology in discovering biologically meaningful clusters.},
	author = {Monti, Stefano and Tamayo, Pablo and Mesirov, Jill and Golub, Todd},
	da = {2003/07/01},
	date-added = {2021-12-02 13:41:40 +0100},
	date-modified = {2021-12-02 13:41:40 +0100},
	doi = {10.1023/A:1023949509487},
	id = {Monti2003},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {91--118},
	title = {Consensus Clustering: A Resampling-Based Method for Class Discovery and Visualization of Gene Expression Microarray Data},
	ty = {JOUR},
	volume = {52},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1023/A:1023949509487}}

@article{COSA,
	abstract = {Summary.  A new procedure is proposed for clustering attribute value data. When used in conjunction with conventional distance-based clustering algorithms this procedure encourages those algorithms to detect automatically subgroups of objects that preferentially cluster on subsets of the attribute variables rather than on all of them simultaneously. The relevant attribute subsets for each individual cluster can be different and partially (or completely) overlap with those of other clusters. Enhancements for increasing sensitivity for detecting especially low cardinality groups clustering on a small subset of variables are discussed. Applications in different domains, including gene expression arrays, are presented.},
	author = {Friedman, Jerome H. and Meulman, Jacqueline J.},
	doi = {https://doi.org/10.1111/j.1467-9868.2004.02059.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2004.02059.x},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Bioinformatics, Clustering on variable subsets, Distance-based clustering, Feature selection, Gene expression microarray data, Genomics, Inverse exponential distance, Mixtures of numeric and categorical variables, Targeted clustering},
	number = {4},
	pages = {815-849},
	title = {Clustering objects on subsets of attributes (with discussion)},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.02059.x},
	volume = {66},
	year = {2004},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.02059.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2004.02059.x}}

@article{rCOSA,
	abstract = {rCOSA is a software package interfaced to the R language. It implements statistical techniques for clustering objects on subsets of attributes in multivariate data. The main output of COSA is a dissimilarity matrix that one can subsequently analyze with a variety of proximity analysis methods. Our package extends the original COSA software (Friedman and Meulman, 2004) by adding functions for hierarchical clustering methods, least squares multidimensional scaling, partitional clustering, and data visualization. In the many publications that cite the COSA paper by Friedman and Meulman (2004), the COSA program is actually used only a small number of times. This can be attributed to the fact that this original implementation is not very easy to install and use. Moreover, the available software is out-of-date. Here, we introduce an up-to-date software package and a clear guidance for this advanced technique. The software package and related links are available for free at: https://github.com/mkampert/rCOSA.},
	author = {Kampert, Maarten M. and Meulman, Jacqueline J. and Friedman, Jerome H.},
	da = {2017/10/01},
	date-added = {2022-05-04 19:00:47 +0200},
	date-modified = {2022-05-04 19:00:47 +0200},
	doi = {10.1007/s00357-017-9240-z},
	id = {Kampert2017},
	isbn = {1432-1343},
	journal = {Journal of Classification},
	number = {3},
	pages = {514--547},
	title = {rCOSA: A Software Package for Clustering Objects on Subsets of Attributes},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s00357-017-9240-z},
	volume = {34},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s00357-017-9240-z}}
