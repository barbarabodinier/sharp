@misc{ourstabilityselection,
      title={Automated calibration for stability selection in penalised regression and graphical models: a multi-OMICs network application exploring the molecular response to tobacco smoking}, 
      author={Barbara Bodinier and Sarah Filippi and Therese Haugdahl Nost and Julien Chiquet and Marc Chadeau-Hyam},
      year={2021},
      eprint={2106.02521},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url = {https://arxiv.org/abs/2106.02521}
}


@article{stabilityselectionMB,
author = {Meinshausen, Nicolai and Bühlmann, Peter},
title = {Stability selection},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {72},
number = {4},
pages = {417-473},
keywords = {High dimensional data, Resampling, Stability selection, Structure estimation},
doi = {10.1111/j.1467-9868.2010.00740.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00740.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00740.x},
abstract = {Summary.  Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
year = {2010}
}


@article{stabilityselectionSS,
author = {Shah, Rajen D. and Samworth, Richard J.},
title = {Variable selection with error control: another look at stability selection},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {75},
number = {1},
pages = {55-80},
keywords = {Complementary pairs stability selection, r-concavity, Subagging, Subsampling, Variable selection},
doi = {10.1111/j.1467-9868.2011.01034.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2011.01034.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2011.01034.x},
abstract = {Summary.  Stability selection was recently introduced by Meinshausen and Bühlmann as a very general technique designed to improve the performance of a variable selection algorithm. It is based on aggregating the results of applying a selection procedure to subsamples of the data. We introduce a variant, called complementary pairs stability selection, and derive bounds both on the expected number of variables included by complementary pairs stability selection that have low selection probability under the original procedure, and on the expected number of high selection probability variables that are excluded. These results require no (e.g. exchangeability) assumptions on the underlying model or on the quality of the original selection procedure. Under reasonable shape restrictions, the bounds can be further tightened, yielding improved error control, and therefore increasing the applicability of the methodology.},
year = {2013}
}


@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}


@article{sparsePCASVD,
title = {Sparse principal component analysis via regularized low rank matrix approximation},
journal = {Journal of Multivariate Analysis},
volume = {99},
number = {6},
pages = {1015-1034},
year = {2008},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2007.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X07000887},
author = {Haipeng Shen and Jianhua Z. Huang},
keywords = {Dimension reduction, High-dimension-low-sample-size, Regularization, Singular value decomposition, Thresholding},
abstract = {Principal component analysis (PCA) is a widely used tool for data analysis and dimension reduction in applications throughout science and engineering. However, the principal components (PCs) can sometimes be difficult to interpret, because they are linear combinations of all the original variables. To facilitate interpretation, sparse PCA produces modified PCs with sparse loadings, i.e. loadings with very few non-zero elements. In this paper, we propose a new sparse PCA method, namely sparse PCA via regularized SVD (sPCA-rSVD). We use the connection of PCA with singular value decomposition (SVD) of the data matrix and extract the PCs through solving a low rank matrix approximation problem. Regularization penalties are introduced to the corresponding minimization problem to promote sparsity in PC loadings. An efficient iterative algorithm is proposed for computation. Two tuning parameter selection methods are discussed. Some theoretical results are established to justify the use of sPCA-rSVD when only the data covariance matrix is available. In addition, we give a modified definition of variance explained by the sparse PCs. The sPCA-rSVD provides a uniform treatment of both classical multivariate data and high-dimension-low-sample-size (HDLSS) data. Further understanding of sPCA-rSVD and some existing alternatives is gained through simulation studies and real data examples, which suggests that sPCA-rSVD provides competitive results.}
}


@article{sparsePCA,
author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
title = {Sparse Principal Component Analysis},
journal = {Journal of Computational and Graphical Statistics},
volume = {15},
number = {2},
pages = {265-286},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/106186006X113430},
URL = { 
        https://doi.org/10.1198/106186006X113430   
},
eprint = { 
        https://doi.org/10.1198/106186006X113430   
}
}


@article{sparsePLS,
   author = {KA, Lê Cao and Rossouw, D. and Robert-Granié, C. and Besse, P.},
   title = {A sparse PLS for variable selection when integrating omics data},
   journal = {Stat Appl Genet Mol Biol},
   volume = {7},
   number = {1},
   pages = {Article 35},
   ISSN = {1544-6115},
   DOI = {10.2202/1544-6115.1390},
   year = {2008},
   type = {Journal Article}
}



@article{sparsegroupPLS,
   author = {Liquet, B. and de Micheaux, P. L. and Hejblum, B. P. and Thiébaut, R.},
   title = {Group and sparse group partial least square approaches applied in genomics context},
   journal = {Bioinformatics},
   volume = {32},
   number = {1},
   pages = {35-42},
   ISSN = {1367-4803},
   DOI = {10.1093/bioinformatics/btv535},
   year = {2016},
   type = {Journal Article}
}
